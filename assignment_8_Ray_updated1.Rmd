
---
title: "Assignment 8"
output: html_document
---

##Assignment 8
##Jennifer Ray

##For this assignment, you'll be using the lemons dataset, which is a subset of the dataset used for a Kaggle competition described here: https://www.kaggle.com/c/DontGetKicked/data. Your job is to predict which cars are most likely to be lemons.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries}
library(tidyverse)
library(knitr)
library(modelr)
library(caret)
library(pander)
library(forcats)
```

```{r data}
training <- read_csv("C:/Users/rayjm2/Desktop/Scripts/Week 8/DontGetKicked/training.csv")
```

```{r data}
test <- read_csv("C:/Users/rayjm2/Desktop/Scripts/Week 8/DontGetKicked/test.csv")
```

##Complete the following steps.
##1. Calculate the proportion of lemons in the training dataset using the IsBadBuy variable. *Use Trainig Data

```{r}
table(training$IsBadBuy)
```


##2. Calculate the proportion of lemons by Make. *Use Trainig Data

```{r descriptives}
#Cross Tabs

training%>%
  count(IsBadBuy)%>% # Count numbers 
  mutate(p=prop.table(n))%>% #mutate for proportions using prop.table
  kable(format="markdown") # output to table

```

```{r Conditional Mean}
#Predictions using Conditional Means
make <- training%>%group_by(Make)%>%
  summarise(mean(IsBadBuy))
pander(make)
```

##3. Now, predict the probability of being a lemon using a linear model (lm(y~x), with covariates of your choosing from the training dataset. *Use Trainig Data

```{r linear_model}
# Linear model
lm_mod<-lm(IsBadBuy~
             Color+
             Make,
           data=training,y=TRUE,na.exclude=TRUE);summary(lm_mod)
```


##Now use Testing Data for the next questions.

```{r data}
test <- read_csv("C:/Users/rayjm2/Desktop/Scripts/Week 8/DontGetKicked/test.csv")
```

##4. Make predictions from the linear model. 


```{r LM Predictions, echo=TRUE}
training <- training%>%
  add_predictions(lm_mod)%>% #Add in predictions from the model
  rename(pred_lm=pred)%>% #rename to be predictions from ols (lm)
  mutate(pred_lm_out=ifelse(pred_lm>=.5,1,0)) #assign output if >= to .5 1=yes and 0=no
```

```{r LM Prediction Table, echo=TRUE}
predlm_table <- table(training$IsBadBuy,training$pred_lm_out)
predlm_table
```

```{r Probability Table Clean-up, echo=TRUE}
prop.table(predlm_table)
rownames(predlm_table) <- c("Predicted 0","Predicted 1")
colnames(predlm_table) <- c("Actually 0", "Actually1")
```



##5. Now, predict the probability of being a lemon using a logistic regression(glm(y~x,family=binomial(link="logit"))), again using covariates of your choosing.  


```{r}
#Logisitic model
logitlem_mod<-glm(IsBadBuy~
             Make+
             Color,
             data=training,
            na.action=na.exclude,
            family=binomial(link="logit"),
               y=TRUE)
summary(logitlem_mod)
```

##6. Make predictions from the logit model. Make sure these are probabilities.

```{r}
training<-training%>%
  mutate(pred_logitlem=predict(logitlem_mod,type="response"))
```

##7. Create a confusion matrix from your linear model and your logit model.

```{r}
training<-training%>%
    mutate(pred_logitlem_out=ifelse(pred_logitlem>=.3,1,0))
training<-training%>%
    mutate(pred_logitlem_out=as.factor(pred_logitlem_out))
training<-training%>%
    mutate(IsBadBuy=as.factor(IsBadBuy))
```

Now we create a confusion matrix to see how we did. 
```{r}
confusionMatrix(data=as.factor(training$pred_logitlem_out),reference=as.factor(training$IsBadBuy))
```



